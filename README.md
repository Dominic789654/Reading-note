# üìö Reading Notes

Welcome to my personal reading notes repository. Here, you'll find summaries and insights from various articles, papers, and books I've read.

## Table of Contents

| üìñ Reading Note | üìÖ Date | üè∑Ô∏è Keywords | üîó Link |
|:---------------:|:-------:|:-----------:|:-------:|
| Successful Language Model Evaluations | 2024-05-25 | Evaluation, Benchmarks, Tutorial | [Read more](./Evaluation/successful_language_model_eval.md) |
| From Explicit CoT to Implicit CoT | 2024-05-26 |  Implicit Prompt, CoT, Reasoning | [Read more](./CoT/Explicit_CoT.md) |
| LongAlign: Long Context Alignment | 2024-05-27 | Long Context, Alignment, Benchmark | [Read more](./Long-Context/LongAlign.md) |
| LongBench: Long Context Benchmark | 2024-05-28 | Long Context, Benchmark, Multitask | [Read more](./Long-Context/LongBench.md) |
|Advancing ... Long-Context LLMs: A Comprehensive Survey | 2024-05-29 | Long-Context, Survey | [Read more](./Long-Context/Advancing_Transformer_Architecture_in_Long-Context_Large_Language_Models_A_Comprehensive_Survey.md) |
|üëç Label Words are Anchors | 2024-05-29 | ICL, Anchors, Information Flow | [Read more](./ICL/Label_Words_are_Anchos.md) |
| Batch Prompting: Efficient Inference with Large Language Model APIs | 2024-05-30 | Batch Prompting, CoT | [Read more](./CoT/Batch_prompt.md) |
| Dataset Growth | 2024-05-31 | Dataset, clean | [Read more](./Dataset/Dataset_Growth.md) |  
| [Todo] Banishing LLM Hallucinations Requires Rethinking Generalization  | - | - | - |
| [Todo] Anthropic papers | - | - | [Link](https://www.anthropic.com/research) |
| [Todo] Prompt Cache | - | - | [Link](https://arxiv.org/pdf/2311.04934)  |
| Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference | 2024-06-17 | Inference, Long-Context | [Read more](./Long-Context/Quest.md) |
| Optimizing AI Inference at Character.AI | 2024-06-18 | Inference, Optimizing, Character.AI | [Read more](./Inference/optim_Character_AI.md) |
| SnapKV: Compressing KV Cache in LLMs | 2024-07-01 | Inference, SnapKV, Compressing KV Cache | [Read more](./Inference/snapKV.md) |
| Efficient Streaming Language Models with Attention Sinks | 2024-07-01 | Inference, StreamingLLM, Attention Sinks | [Read more](./Inference/StreamingLLM.md) |
| H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models | 2024-07-01 | Inference, H2O, Heavy-Hitter Oracle | [Read more](./Inference/H2o.md) |
| ChunkAttention | 2024-07-01 | Inference, ChunkAttention, Attention | [Read more](./Inference/ChunkAttention.md) |
| ADORE | 2024-07-07 | Inference, ADORE, Attention | [Read more](./Inference/ADORE.md) |
| Reducing Transformer Key-Value Cache Size with Cross-Layer Attention | 2024-07-10 | Inference, CLA, Cross-Layer Attention | [Read more](./Inference/CLA.md) |
| Layer-Condensed KV Cache for Efficient Inference of Large Language Models | 2024-07-10 | Inference, Layer-Condensed KV Cache | [Read more](./Inference/Layer-Condensed_KV_Cache.md) |
| MiniCache: KV Cache Compression in Depth Dimension for Large Language Models | 2024-07-10 | Inference, MiniCache, KV Cache Compression | [Read more](./Inference/MiniCache.md) |
| FastGen: Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs | 2024-07-11 | Inference, FastGen, Adaptive KV Cache Compression | [Read more](./Inference/FastGen.md) |
| PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference | 2024-07-11 | Inference, PyramidInfer, KV Cache Compression | [Read more](./Inference/PyramidInfer.md) |
| HUMAN-LIKE EPISODIC MEMORY FOR INFINITE CONTEXT LLMS | 2024-07-15 | Inference, Episodic Memory, Infinite Context | [Read more](./Inference/EM-LLM.md) |
| D2O: Dynamic Discriminative Operations for Efficient Generative Inference of Large Language Models | 2024-07-21 | Inference, KV Cache merge | [Read more](./Inference/D2O.md) |
| Mixture of Sparse Attention for Automatic Large Language Model Compression | 2024-07-22 | Inference, MoA, Sparse Attention | [Read more](./Inference/MoA.md) |
| LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference | 2024-07-23 | Inference, LazyLLM, Token Pruning | [Read more](./Inference/LazyLLM.md) |
| SampleAttention: Adaptive Structured Sparse Attention for Long Context LLMs | 2024-07-24 | Inference, SampleAttention, Structured Sparse Attention | [Read more](./Inference/SampleAttention.md) |
| ThinK: Thinner Key Cache by Query-Driven Pruning | 2024-08-09 | Inference, ThinK, Query-Driven Pruning | [Read more](./Inference/ThinK.md) |
| MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer Decoding | 2024-08-09 | Inference, MLKV, Multi-Layer Key-Value Heads | [Read more](./Inference/MLKV.md) |
| Palu: Compressing KV-Cache with Low-Rank Projection | 2024-08-11 | Inference, Palu, Low-Rank Projection | [Read more](./Inference/Palu.md) |
| Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time | 2024-08-19 | Inference, KV Cache Compression | [Read more](./Inference/Scissorhands.md) |
| AdaKV: Adaptive Budget Allocation for Efficient KV Cache Eviction in Large Language Models | 2024-08-29 | Inference, AdaKV, KV Cache Eviction | [Read more](./Inference/AdaKV.md) |
| Word Embeddings: The Hidden Steers of Language Models | 2024-10-11 | Embedding, LM-Steer | [Read more](./Embedding/LM-steer.md) |
| ToolGen: Revolutionizing AI Tool Interaction | 2024-10-12 | Agent, ToolGen | [Read more](./Agent/ToolGen.md) |
| Synergy of Thoughts (SoT): Enhancing Reasoning Efficiency in Hybrid Language Models | 2024-10-12 | CoT, SoT | [Read more](./CoT/SoT.md) |
| AgentSquare: Revolutionizing LLM Agent Design with Modular Search | 2024-10-12 | Agent, AgentSquare | [Read more](./Agent/AgentSquare.md) |

---

Feel free to explore and share your thoughts!