# üìö Reading Notes

Welcome to my personal reading notes repository. Here, you'll find summaries and insights from various articles, papers, and books I've read.

## Table of Contents

| üìñ Reading Note | üìÖ Date | üè∑Ô∏è Keywords | üîó Link |
|:---------------:|:-------:|:-----------:|:-------:|
| Successful Language Model Evaluations | 2024-05-25 | Evaluation, Benchmarks, Tutorial | [Read more](./Evaluation/successful_language_model_eval.md) |
| From Explicit CoT to Implicit CoT | 2024-05-26 |  Implicit Prompt, CoT, Reasoning | [Read more](./CoT/Explicit_CoT.md) |
| LongAlign: Long Context Alignment | 2024-05-27 | Long Context, Alignment, Benchmark | [Read more](./Long-Context/LongAlign.md) |
| LongBench: Long Context Benchmark | 2024-05-28 | Long Context, Benchmark, Multitask | [Read more](./Long-Context/LongBench.md) |
|Advancing ... Long-Context LLMs: A Comprehensive Survey | 2024-05-29 | Long-Context, Survey | [Read more](./Long-Context/Advancing_Transformer_Architecture_in_Long-Context_Large_Language_Models_A_Comprehensive_Survey.md) |
|üëç Label Words are Anchors | 2024-05-29 | ICL, Anchors, Information Flow | [Read more](./ICL/Label_Words_are_Anchos.md) |
| Batch Prompting: Efficient Inference with Large Language Model APIs | 2024-05-30 | Batch Prompting, CoT | [Read more](./CoT/Batch_prompt.md) |
| Dataset Growth | 2024-05-31 | Dataset, clean | [Read more](./Dataset/Dataset_Growth.md) |  
| [Todo] Banishing LLM Hallucinations Requires Rethinking Generalization  | - | - | - |
| [Todo] Anthropic papers | - | - | [Link](https://www.anthropic.com/research) |
| [Todo] Prompt Cache | - | - | [Link](https://arxiv.org/pdf/2311.04934)  |
| Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference | 2024-06-17 | Inference, Long-Context | [Read more](./Long-Context/Quest.md) |
| Optimizing AI Inference at Character.AI | 2024-06-18 | Inference, Optimizing, Character.AI | [Read more](./Inference/optim_Character_AI.md) |
| SnapKV: Compressing KV Cache in LLMs | 2024-07-01 | Inference, SnapKV, Compressing KV Cache | [Read more](./Inference/snapKV.md) |
| Efficient Streaming Language Models with Attention Sinks | 2024-07-01 | Inference, StreamingLLM, Attention Sinks | [Read more](./Inference/StreamingLLM.md) |
| H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models | 2024-07-01 | Inference, H2O, Heavy-Hitter Oracle | [Read more](./Inference/H2o.md) |
| ChunkAttention | 2024-07-01 | Inference, ChunkAttention, Attention | [Read more](./Inference/ChunkAttention.md) |
| ADORE | 2024-07-07 | Inference, ADORE, Attention | [Read more](./Inference/ADORE.md) |
| Reducing Transformer Key-Value Cache Size with Cross-Layer Attention | 2024-07-10 | Inference, CLA, Cross-Layer Attention | [Read more](./Inference/CLA.md) |
| Layer-Condensed KV Cache for Efficient Inference of Large Language Models | 2024-07-10 | Inference, Layer-Condensed KV Cache | [Read more](./Inference/Layer-Condensed_KV_Cache.md) |
| MiniCache: KV Cache Compression in Depth Dimension for Large Language Models | 2024-07-10 | Inference, MiniCache, KV Cache Compression | [Read more](./Inference/MiniCache.md) |
| FastGen: Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs | 2024-07-11 | Inference, FastGen, Adaptive KV Cache Compression | [Read more](./Inference/FastGen.md) |
| PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference | 2024-07-11 | Inference, PyramidInfer, KV Cache Compression | [Read more](./Inference/PyramidInfer.md) |
| HUMAN-LIKE EPISODIC MEMORY FOR INFINITE CONTEXT LLMS | 2024-07-15 | Inference, Episodic Memory, Infinite Context | [Read more](./Inference/EM-LLM.md) |
| D2O: Dynamic Discriminative Operations for Efficient Generative Inference of Large Language Models | 2024-07-21 | Inference, KV Cache merge | [Read more](./Inference/D2O.md) |
---

Feel free to explore and share your thoughts!